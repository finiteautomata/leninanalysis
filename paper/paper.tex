%% PNAStwoS.tex
%% Sample file to use for PNAS articles prepared in LaTeX
%% For two column PNAS articles
%% Version: Apr 15, 2008 
 

%% BASIC CLASS FILE
\documentclass{pnastwo}


%% ADDITIONAL OPTIONAL STYLE FILES
%\usepackage[dvips]{graphicx}
%\usepackage{pnastwof}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage[utf8]{inputenc}
\usepackage[activeacute, spanish]{babel}
\usepackage[dvips]{graphicx}
\selectlanguage{spanish}


%% OPTIONAL MACRO DEFINITIONS
\def\s{\sigma}


%%%%%%%%%%%%
%% For PNAS Only:
\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}
%\setcounter{page}{2687} %Set page number here if desired
%%%%%%%%%%%%

\begin{document}

\title{An analysis of Lenin's work: Revolution, war, and fruta}

\author{Leonardo Lázzaro,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
Julián Peller,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
Juan Manuel Pérez,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
}

\contributor{Submitted to Proceedings of the National Academy of Sciences
of the United States of America}

\maketitle

\begin{article}
\begin{abstract}
In order to discover how much related were Lenin Works to Russian History in nearly 20th century, we use the information value concept to extract the most important words from them. Therefore, we find out how far are them from certain concepts such as
war, revolution, theory, etc. We expected this concepts to be nearer and farther to concepts 
treated in Lenin's works depending on the events ranging from 1900 to 1924.
\end{abstract}

\keywords{information value | natural language processing | lenin }

\abbreviations{NLP}

\section{Introduction}

\dropcap{I}n this article we study distances in metric spaces consisting on words, and how we can
represent a text (an ordered set of words) as a smaller subset of their most representative words.

In \cite{DARWIN}, Zanette and Montemurro defined the information value of a word in a text. This is
a real value that quantifies how much information that word carries according to the text. Let $T$ be a 
text, and $T'$ a random shuffled version of $T$, and $w$ a word appearing in $T$. 
By $S_T(w)$ we denote the Shannon Entropy of $w$ with respect to $T$ (See appendix for more info). 

The information value of $w$ in $T$ is 

\begin{equation}
  IV_T(w) = f_w* | S_T(w) - S_{T'}(w) | 
\end{equation}

where $f_w$ is the frequency of appearance of $w$ in $T$. This value captures, in some way, whether a word
has a distribution in text that is not exactly uniform; an order of the word in the text that brings some sense to it.

Having these value, we can pull the most representative words out of a text by selecting those of greater information value.

\section{Method}
\subsection{Text Corpus}
We gathered Lenin's work from the "Marxists Internet Archive". This proved to be a difficult task as the works were not in a common HTML format. The tool we used to do this is Scrapy, a very comprehensive Python crawling framework.

In order to reduce noise in the corpus, we filtered all the letters and works that were comparatively little. Our information value theory doesn't work very well with texts with little repetition of words, so we decided to remove texts with less than 2000 words. 

\begin{figure}
\centerline{\includegraphics[width=.4\textwidth]{figures/tokens_per_year.eps}}
\caption{Number of tokens per year}\label{afoto}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=.4\textwidth]{figures/hist_tokens_small_works.eps}}
\caption{Distribution of works with respect to size (small works)}\label{afoto}
\end{figure}


\begin{figure}
\centerline{\includegraphics[width=.4\textwidth]{figures/hist_tokens_big_works.eps}}
\caption{Distribution of works with respect to size (big works)}\label{afoto}
\end{figure}


%*Números (obras totales obtenidas, promedio tokens)
\subsection{Information Value Framework}
%
%*Aclarar settings de Zannette
%*Maximización, ventanas
%*Cálculo paralelo
%*Ajustes de parametros para que concuerden con los resultados de zanette
%*doc.top_words()
%  representación como set de palabras ponderadas por documento. 

\subsection{Wordnet analyzer}
Consideremos el siguiente problema: ¿cómo calcular la similaridad o distancia de un documento a un cierto concepto? El enfoque que utilizamos en este trabajo es el de realizar una representación del documento como un conjunto de palabras, y en base a esto usar una noción topológica de similaridad/distancia .

% *WordnetAnalyzer
%   *Synset
%   *path, lch, wup
% *judge_word
% *judge_doc
% *judge_year
% Dado un conjunto de palabras, definimos la distancia de una palabra a este conjunto como 
% 
S(x, C) = max { ... }

Usamos la distancia path.

Ahora, dado un documento, calculamos

\section{Results}
\subsection{Resultados sobre obras}
* Tabla de top words de Estado y Revolución
* Distancia a algunos conceptos de estado y revolución
\subsection{Resultados sobre años}
*
* x = {años}, y = {concept value}

\section{Discussion}
* Zanette diferia en valores absolutos con los resultados nuestros. Moby Dick. 

\section{Acknowledgments}
\section{Literature cited}



\section{Analysis of almost sharp fronts}
We begin our analysis on almost sharp fronts for the
quasi-geostrophic equation recalling the notion of weak solution.

For these solutions we have the following


\subsection{Data Sources}
We are interested in studying the evolution of almost sharp fronts
for the QG equation. These are weak solutions of the equation with
large gradient ($\sim \displaystyle{ \inlinefrac{1}{\delta}}$, where $2
\delta$ is the thickness of the transition layer for $\theta$).



\begin{materials}
\section{Digital RNA SNP Analysis} A real-time PCR assay was designed
to amplify {\it PLAC4} mRNA, with the two SNP alleles being discriminated
by TaqMan probes. {\it PLAC4} mRNA concentrations were quantified in
extracted RNA samples followed by dilutions to approximately one target
template molecule of either type (i.e., either allele) per well.
Details are given in the {\it SI Materials and Methods}.

\section{Digital RCD Analysis} Extracted DNA was quantified by
spectrophotometry (NanoDrop Technologies, Wilmington, DE) and diluted to a
concentration of 
approximately one target template from either chr21 or ch1 per well.
\end{materials}

\appendix[Entropy of a word in a text]

Let's recall from \cite{DARWIN} and \cite{ENTROPIC} the concept of information value of a word. 
Let $T$ be a text, which has been split into $P$ pieces $T_1, T_2, \dots, T_P$, $w$ be a word in $T$,
and $f_i$ the frequency of appearance of $w$ in $T_i$.

For each part $1 \leq i \leq P$, we could define:

\begin{equation}
  p_i = \frac{f_i}{\Sigma_{j=1}^{P}f_i}
\end{equation}

This quantity stands for probability of finding $w$ in $T_i$, given that it is present in the text. So, 
we define the Shannon information entropy of $w$ in $T$ as

\begin{equation}
  S = -\frac{1}{ln P}\sum_{i=1}^{P}p_i ln p_i
\end{equation}


\appendix[Estimating the Spectral Norm of a Matrix]
In this appendix we describe a method for the estimation of the spectral norm
of matrix $A$. The method does not require access to the individual
entries of $A$; it requires only applications of $A$ and $A$* to vectors.
It is a version of the classical power method. Its probabilistic
analysis summarized below was introduced fairly recently in refs. 13
and 14. This appendix is included here for completeness.


\appendix
This is an example of an appendix without a title.

\begin{acknowledgments}
This work was partially supported by 
Spanish Ministry of Science and Technology Grant BFM2002-02042 (to D.C. and
J.L.R.) and by National Science Foundation Grand DMS-0245242 (to C.F.).
\end{acknowledgments}



\begin{thebibliography}{10}
\bibitem{DARWIN}
M.A. Montemurro and D. H. Zanette, {\em The statistics of meaning: Darwin, Gibbon and Moby Dick}, Significance, Dec. 2009, 165-169.
\bibitem{ENTROPIC}
M.A. Montemurro and D. H. Zanette, {\em Entropic analysis of the role of words in literary texts}, Adv. Complex Systems 5, September 27th 2001
\end{thebibliography}
\end{article}


\begin{table}[h]
\caption{Repeat length of longer allele by age of onset class.
This is what happens when the text continues.}
\begin{tabular}{@{\vrule height 10.5pt depth4pt  width0pt}lrcccc}
&\multicolumn5c{Repeat length}\\
\noalign{\vskip-11pt}
Age of onset,\\
\cline{2-6}
\vrule depth 6pt width 0pt years&\multicolumn1c{\it n}&Mean&SD&Range&Median\\
\hline
Juvenile, 2$-$20&40&60.15& 9.32&43$-$86&60\\
Typical, 21$-$50&377&45.72&2.97&40$-$58&45\\
Late, $>$50&26&41.85&1.56&40$-$45&42\tablenote{The no. of wells for all samples was 384. Genotypes were
determined by mass spectrometric assay. The $m_t$ value indicates the
average number of wells positive for the over represented allele.}
\\
\hline
\end{tabular}
\end{table}


\begin{table*}[ht]
\caption{Summary of the experimental results}
\begin{tabular*}{\hsize}
{@{\extracolsep{\fill}}rrrrrrrrrrrrr}
\multicolumn{3}{l}{Parameters}&
\multicolumn{5}{c}{Averaged Results}&
\multicolumn{5}{c}{Comparisons}\cr
\hline
\multicolumn1c{$n$}&\multicolumn1c{$S^*_{MAX}$}&
\multicolumn1c{$t_1$}&\multicolumn1c{\ $r_1$}&
\multicolumn1c{\ $m_1$}&\multicolumn1c{$t_2$}&
\multicolumn1c{$r_2$}&\multicolumn1c{$m_2$}
&\multicolumn1c{$t_{lb}$}&\multicolumn1c{\ \ $t_1/t_2$}&
$r_1/r_2$&$m_1/m_2$&
$t_1/t_{lb}$\cr
\hline
10\tablenote{Stanford Synchrotron Radiation Laboratory (Stanford University,
Stanford, CA)}&1\quad &4&.0007&4&4&.0020&4&4&1.000&.333&1.000&1.000\cr
10\tablenote{$R_{\rm FREE}=R$ factor for the $\sim 5$\% of the randomly
chosen unique ref\/lections not used in the ref\/inement.}&5\quad &50&.0008&8&50&.0020&12&49&.999&.417&.698&1.020\cr
100\tablenote{Calculated for all observed data}&20\quad &2840975&.0423&95&2871117&.1083&521&---&
.990&.390&.182&---\ \ \cr
\hline
\end{tabular*}
\end{table*}






\end{document}


