%% PNAStwoS.tex
%% Sample file to use for PNAS articles prepared in LaTeX
%% For two column PNAS articles
%% Version: Apr 15, 2008 
 

%% BASIC CLASS FILE
\documentclass{pnastwo}

%% ADDITIONAL OPTIONAL STYLE FILES
%\usepackage[dvips]{graphicx}
%\usepackage{pnastwof}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage[utf8]{inputenc}
\usepackage[activeacute, spanish]{babel}
\selectlanguage{spanish}

%% OPTIONAL MACRO DEFINITIONS
\def\s{\sigma}


%%%%%%%%%%%%
%% For PNAS Only:
\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}
%\setcounter{page}{2687} %Set page number here if desired
%%%%%%%%%%%%

\begin{document}

\title{An analysis of Lenin's work: Revolution, war, and fruta}

\author{Leonardo Lázzaro,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
Julián Peller,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
Juan Manuel Pérez,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
}

\contributor{Submitted to Proceedings of the National Academy of Sciences
of the United States of America}

\maketitle

\begin{article}
\begin{abstract}
In order to discover how much related were Lenin Works to Russian History in nearly 20th century, we use the information value concept to extract the most important words from them. Therefore, we find out how far are them from certain concepts such as
war, revolution, theory, etc. We expected this concepts to be nearer and farther to concepts 
treated in Lenin's works depending on the events ranging from 1900 to 1924.
\end{abstract}

\keywords{information value | natural language processing | lenin }

\abbreviations{NLP}

\section{Introduction}

\dropcap{I}n this article we study distances in metric spaces consisting on words, and how we can
represent a text (an ordered set of words) as a smaller subset of their most representative words.

In \cite{DARWIN}, Zanette and Montemurro defined the information value of a word in a text. This is
a real value that quantifies how much information that word carries according to the text. Let $T$ be a 
text, and $T'$ a random shuffled version of $T$, and $w$ a word appearing in $T$. 
By $S_T(w)$ we denote the Shannon Entropy of $w$ with respect to $T$ (See appendix for more info). 

The information value of $w$ in $T$ is 

\begin{equation}
  IV_T(w) = f_w* | S_T(w) - S_{T'}(w) | 
\end{equation}

where $f_w$ is the frequency of appearance of $w$ in $T$. This value captures, in some way, whether a word
has a distribution in text that is not exactly uniform; an order of the word in the text that brings some sense to it.

Having these value, we can pull the most representative words out of a text by selecting those of greater information value.

\section{Methods}
\subsection{Corpus}
*Scrapping marxists.org
*Filtrado Obras chiquitas (cartas incluidas)
*Números (obras totales obtenidas, promedio tokens)
\subsection{Information Value Framework}
*Aclarar settings de Zannette
*Maximización, ventanas
*Cálculo paralelo
*Ajustes de parametros para que concuerden con los resultados de zanette
*doc.top_words()
  representación como set de palabras ponderadas por documento. 

\subsection{Wordnet analyzer}
Consideremos el siguiente problema: ¿cómo calcular la similaridad o distancia de un documento a un cierto concepto? El enfoque que utilizamos en este trabajo es el de realizar una representación del documento como un conjunto de palabras, y en base a esto usar una noción topológica de similaridad/distancia .

*WordnetAnalyzer
  *Synset
  *path, lch, wup
*judge_word
*judge_doc
*judge_year
Dado un conjunto de palabras, definimos la distancia de una palabra a este conjunto como 

S(x, C) = max { ... }

Usamos la distancia path.

Ahora, dado un documento, calculamos

\section{Results}
\subsection{Resultados sobre obras}
* Tabla de top words de Estado y Revolución
* Distancia a algunos conceptos de estado y revolución
\subsection{Resultados sobre años}
*
* x = {años}, y = {concept value}

\section{Discussion}
* Zanette diferia en valores absolutos con los resultados nuestros. Moby Dick. 

\section{Acknowledgments}
\section{Literature cited}

\begin{equation} (-\triangle)^{\f12}\psi=\theta \, ,\label{qg3} \end{equation}
For simplicity we are considering fronts on the cylinder, i.e. we
take $(x,y)$ in $\mathbb{R}/_{\displaystyle{\mathbb{Z}}}\times
\mathbb{R}$. In this setting we define $-\triangle^{-\f12}$ that
comes from inverting the third equation by convolution with the
kernel. To avoid irrelevant considerations at $\infty$ we
will take $\eta$ to be compactly supported
\[
\mfrac{\chi(u,v)}{(u^2+v^2)^{\f12}}+\eta(u,v)
\]
where $\chi (x,y)\ \epsilon\ C^{\infty}_{0},\,\,
\chi(x,y)=1 \hspace{8pt} \mbox{in} \hspace{5pt} |x-y|\leq r \,\,
\mbox{and}\,\, supp \chi$ is contained in $  \{|x-y|\leq R\} $
with $0<r<R<\frac{1}{2}$. Also $ \eta\ \epsilon\ C^{\infty}_0$,
$\eta(0,0)=0 $.


Recently one of the authors has obtained the equation for the
evolution of sharp fronts (in the periodic setting), proving its
local well-posedness for that equation. This is a problem in contour
dynamics. Contour dynamics for other fluid equations has been
studied extensively. 


\section{Analysis of almost sharp fronts}
We begin our analysis on almost sharp fronts for the
quasi-geostrophic equation recalling the notion of weak solution.

For these solutions we have the following


\subsection{Data Sources}
We are interested in studying the evolution of almost sharp fronts
for the QG equation. These are weak solutions of the equation with
large gradient ($\sim \displaystyle{ \inlinefrac{1}{\delta}}$, where $2
\delta$ is the thickness of the transition layer for $\theta$).

\section{Discussion}
\subsection{Cylindrical Case}
We are going to consider the cylindrical case here. We consider a
transition layer of thickness smaller than $2\delta$ in which
 $\theta$ changes from 0 to 1. (see Figure ).That means we are considering $\theta$  of the form
\[
\theta  = 1   \mbox{ if }\quad  y\geq \varphi(x,t)+\delta
\]
\[
\theta \mbox{ bounded} \mbox{ if }\quad |\varphi(x,t)-y|\leq\delta
\]
\be \theta = 0  \mbox{ if }\quad  y\leq \varphi(x,t)-\delta
\label{theta} \ee
where $\varphi$ is a smooth periodic function and
$0<\delta<\frac{1}{2}$.

%For these solutions we have the following


\begin{remark}
Note that equation \eqref{theta} specifies  the function $\varphi$
up to an error of order $\delta$. Theorem 1 provides an evolution
equation for the function $\varphi$ up to an error of order
$\delta |log \delta|$.
\end{remark}

In order to analyze the evolution of the ``almost-sharp'' front we
substitute the above expression for $\theta$ in the definition of
a weak solution (see). We use the notation $X =O(
Y)$ to indicate that $|X|\leq C |Y|$ where the constant $C$
depends only on $\|\theta\|_{L^{\infty}}$, $\|
\nabla\varphi\|_{L^{\infty}}$ and $\|\phi\|_{C^1}$, where $\phi$
is a test function appearing in Definition 1.

We consider the 3 different regions defined by the form on
$\theta$. Since $\theta=0$ in the region I the contribution from
that region is 0, i.e.
\[
  \int_{I\times \mathbb{R}}
 \theta(x,y,t)\, \pr_t \phi
\,(x,y,t) dy dx dt+  \]
\[
   +\int_{I\times \mathbb{R}}
 \theta\,(x,y,t) u(x,y,t)\cdot\nabla\phi\,(x,y,t) dydxdt = 0
\]

As for the region II
\[
\int_{II\times \mathbb{R}} \theta (x,y,t) \pr_t\phi (x,y,t)dx dy
dt = O(\delta)
\]
since $\theta$ is bounded and hence $O(1)$, and the area of the
region II is $O(\delta)$. As for the second term
\[
\int_{II\times \mathbb{R}} u \theta \nabla \phi dx dy dt =
O(\delta log(\delta))
\]

To see this, we fix $t$. We must estimate
\[
\int_{\mathbb{R}^2} u\cdot(\mathbb{1}_{II}\theta\nabla\phi) dx dy
\]
We are left to estimate the terms
\[
\int_{III\times \mathbb{R}} \theta \pr_t\phi dx dy
dt+\int_{III\times \mathbb{R}} \theta u_f\cdot \nabla \phi dx dy
dt=:A+B
\]

\subsection{Observations}
The following observations can be made form the numerical experiments
described in this section, and are consistent with the results
of more extensive experimentation performed by the authors:
\begin{itemize}
\item The CPU times in Tables 1--3 are compatible with the estimates in
formulae {\bf 14}, {\bf 16}, and {\bf 23}.
\item
The precision produced by each of Algorithms {\bf I} and {\bf II}
is similar to that provided by formula {\bf 3}, even when
$\omega_{k+1}$ is close to the machine precision.
\end{itemize}




\begin{materials}
\section{Digital RNA SNP Analysis} A real-time PCR assay was designed
to amplify {\it PLAC4} mRNA, with the two SNP alleles being discriminated
by TaqMan probes. {\it PLAC4} mRNA concentrations were quantified in
extracted RNA samples followed by dilutions to approximately one target
template molecule of either type (i.e., either allele) per well.
Details are given in the {\it SI Materials and Methods}.

\section{Digital RCD Analysis} Extracted DNA was quantified by
spectrophotometry (NanoDrop Technologies, Wilmington, DE) and diluted to a
concentration of 
approximately one target template from either chr21 or ch1 per well.
\end{materials}

\appendix[Entropy of a word in a text]

Let's recall from \cite{DARWIN} and \cite{ENTROPIC} the concept of information value of a word. 
Let $T$ be a text, which has been split into $P$ pieces $T_1, T_2, \dots, T_P$, $w$ be a word in $T$,
and $f_i$ the frequency of appearance of $w$ in $T_i$.

For each part $1 \leq i \leq P$, we could define:

\begin{equation}
  p_i = \frac{f_i}{\Sigma_{j=1}^{P}f_i}
\end{equation}

This quantity stands for probability of finding $w$ in $T_i$, given that it is present in the text. So, 
we define the Shannon information entropy of $w$ in $T$ as

\begin{equation}
  S = -\frac{1}{ln P}\sum_{i=1}^{P}p_i ln p_i
\end{equation}


\appendix[Estimating the Spectral Norm of a Matrix]
In this appendix we describe a method for the estimation of the spectral norm
of matrix $A$. The method does not require access to the individual
entries of $A$; it requires only applications of $A$ and $A$* to vectors.
It is a version of the classical power method. Its probabilistic
analysis summarized below was introduced fairly recently in refs. 13
and 14. This appendix is included here for completeness.


\appendix
This is an example of an appendix without a title.

\begin{acknowledgments}
This work was partially supported by 
Spanish Ministry of Science and Technology Grant BFM2002-02042 (to D.C. and
J.L.R.) and by National Science Foundation Grand DMS-0245242 (to C.F.).
\end{acknowledgments}



\begin{thebibliography}{10}
\bibitem{DARWIN}
M.A. Montemurro and D. H. Zanette, {\em The statistics of meaning: Darwin, Gibbon and Moby Dick}, Significance, Dec. 2009, 165-169.
\bibitem{ENTROPIC}
M.A. Montemurro and D. H. Zanette, {\em Entropic analysis of the role of words in literary texts}, Adv. Complex Systems 5, September 27th 2001
\end{thebibliography}
\end{article}


\begin{table}[h]
\caption{Repeat length of longer allele by age of onset class.
This is what happens when the text continues.}
\begin{tabular}{@{\vrule height 10.5pt depth4pt  width0pt}lrcccc}
&\multicolumn5c{Repeat length}\\
\noalign{\vskip-11pt}
Age of onset,\\
\cline{2-6}
\vrule depth 6pt width 0pt years&\multicolumn1c{\it n}&Mean&SD&Range&Median\\
\hline
Juvenile, 2$-$20&40&60.15& 9.32&43$-$86&60\\
Typical, 21$-$50&377&45.72&2.97&40$-$58&45\\
Late, $>$50&26&41.85&1.56&40$-$45&42\tablenote{The no. of wells for all samples was 384. Genotypes were
determined by mass spectrometric assay. The $m_t$ value indicates the
average number of wells positive for the over represented allele.}
\\
\hline
\end{tabular}
\end{table}


\begin{table*}[ht]
\caption{Summary of the experimental results}
\begin{tabular*}{\hsize}
{@{\extracolsep{\fill}}rrrrrrrrrrrrr}
\multicolumn{3}{l}{Parameters}&
\multicolumn{5}{c}{Averaged Results}&
\multicolumn{5}{c}{Comparisons}\cr
\hline
\multicolumn1c{$n$}&\multicolumn1c{$S^*_{MAX}$}&
\multicolumn1c{$t_1$}&\multicolumn1c{\ $r_1$}&
\multicolumn1c{\ $m_1$}&\multicolumn1c{$t_2$}&
\multicolumn1c{$r_2$}&\multicolumn1c{$m_2$}
&\multicolumn1c{$t_{lb}$}&\multicolumn1c{\ \ $t_1/t_2$}&
$r_1/r_2$&$m_1/m_2$&
$t_1/t_{lb}$\cr
\hline
10\tablenote{Stanford Synchrotron Radiation Laboratory (Stanford University,
Stanford, CA)}&1\quad &4&.0007&4&4&.0020&4&4&1.000&.333&1.000&1.000\cr
10\tablenote{$R_{\rm FREE}=R$ factor for the $\sim 5$\% of the randomly
chosen unique ref\/lections not used in the ref\/inement.}&5\quad &50&.0008&8&50&.0020&12&49&.999&.417&.698&1.020\cr
100\tablenote{Calculated for all observed data}&20\quad &2840975&.0423&95&2871117&.1083&521&---&
.990&.390&.182&---\ \ \cr
\hline
\end{tabular*}
\end{table*}



\end{document}


