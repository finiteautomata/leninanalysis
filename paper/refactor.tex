%% PNAStwoS.tex
%% Sample file to use for PNAS articles prepared in LaTeX
%% For two column PNAS articles
%% Version: Apr 15, 2008 
 

%% BASIC CLASS FILE
\documentclass{pnastwo}

%% ADDITIONAL OPTIONAL STYLE FILES
%\usepackage[dvips]{graphicx}
%\usepackage{pnastwof}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage[utf8]{inputenc}
\usepackage[dvips]{graphicx}
\usepackage{bookmark}
\usepackage{color}
\usepackage{array}
\usepackage{setspace}
\usepackage{longtable}
\usepackage[hmargin=2cm,vmargin=2.5cm]{geometry}
\usepackage{booktabs}
\usepackage{colortbl}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\myrowcolour}{\rowcolor[gray]{0.925}}
  \newcommand{\cheading}[2]{\textbf{#1\hfill #2}}

\newcommand\dq[1]{\textquotedblleft #1\textquotedblright}
\newcommand\sq[1]{\textquoteleft #1\textquoteright}
\newcommand*{\allref}[1]{\ref{#1} \nameref{#1}}

%% OPTIONAL MACRO DEFINITIONS
\def\s{\sigma}


%%%%%%%%%%%%
%% For PNAS Only:
%\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}
%\setcounter{page}{2687} %Set page number here if desired
%%%%%%%%%%%%

\begin{document}

\title{A word-sense representation for documents based on information value and word sense disambiguation}

\author{Leonardo Lazzaro,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
Julián Peller,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
Juan Manuel Pérez,\affil{1}{Universidad de Buenos Aires, Buenos Aires, Argentina},
}

%&\contributor{Submitted to Proceedings of the National Academy of Sciences
%of the United States of America}

\maketitle

\begin{article}
\begin{abstract}
In this project we analyzed some Lenin works in order to identify the presence of some well known topics of his biography. To achieve this, we defined and implemented a simple distance metric between a word-sense and a document and confronted this metric using a evaluation set of word-senses against a reduced set of  documents with well known topics. During this work, we generated an indexed database of Lenin's complete works from a web site\cite{LENIN} and we applied a serie of well known syntactic, semantic and lexicographic tools in order to model the similarity between a document and a word-sense. We applied information value\cite{DARWIN} and a simple algorithm of word sense disambiguation\cite{LESK} based on wordnet\cite{WORDNET} in order to generate the \textit{top senses} (or principal senses) of a document. We compare this \textit{top senses} for some documents of well known topics agains some senses of interest (for example: revolution, war, government) and observed the shape of the results. 
\end{abstract}

\keywords{information value | word sense disambiguation | natural language processing | information retrieval | polysemy }

%\abbreviations{NLP}

\section{Introduction}

\dropcap{I}n this article we propose a simple distance metric between a word-sense and a text document using different well known tools from natural language processing. To achieve this, we extract a hierarchy of the most representative words of a document (\textit{top words}) using the concept of \textit{information value}, a statistical linguistic model based on Shannon's information theory and proposed by Zanette et al. in\cite{DARWIN}. Briefly, information value allows us to represent a document like a ranked set of words. 

But this weighted set of top words has the problem of polysemy, typical in the field of nlp. Polysemy occurs when a term has more than one sense or meaning, for example: the term 'bank' can refer to a financial institution or to the land formation, thus, it is polysemic. In order to resolve this limitation, we implement a basic algorithm of word-sense disambiguation based on wordnet's synsets and we apply it over the top words, using as disambiguation context the whole document. We obtain, then, the \textit{top senses} of a document, this is, a ranked set of wordnet synsets that work like a representation of the given document.

Finally, we utilize different wordnet predefined metrics of similarity between synsets to define our own similarities between the top-sense representation of a document and any given word-sense (wordnet synset). In order to evaluate the behaviour of this metrics, we test the distances of the top-senses representation of some well known documents with very specific topics against a bounded set of relevant evaluation word-senses. 


\section{Method}
\subsection{Text Corpus}

We selected some starred works from Lenin from the "Marxists Internet Archive"\cite{LENIN}. Those works are very different in topic, some of them being strictly phylosophical essays (such as ``Materialism and Empiriocriticism'') and other being more political, such as ``State and Revolution'', and finally an economy essay called ``Imperialism, the highest stage of capitalism''. 

\begin{itemize}
  \item What is to be done(1901)
  \item The Agrarian Programme of Social-Democracy in the First Russian Revolution, 1905-1907 (1907)
  \item Materialism and Empiriocriticism (1908)
  \item Imperialism, the highest stage of capitalism (1916)
  \item State and Revolution (1917)
\end{itemize}

\begin{itemize}
  \item Mention the crawler for the complete lenin works, pointing that finally we didn't used it but that the scrapper is available in the project at github
  \item May be, point out some numbers of the db generated. (total documents, mean tokens per document, etc). It's my opinion, I understand that it's not the main line of the project.
\end{itemize}


%*Números (obras totales obtenidas, promedio tokens)
\subsection{Semantic Analysis}
\medskip
\subsubsection{Information value}

The information value of a word for a given text is defined and tested by Zanette \textit{et al.} in \cite{DARWIN} and \cite{ENTROPIC}.
It is a real value that quantifies how much information a word carries according to the text. Let $T$ be a 
text, and $T'$ a random shuffled version of $T$, and $w$ a word appearing in $T$. 
By $S_T(w)$ we denote the Shannon Entropy of $w$ with respect to $T$ (See appendix for more info). 

The information value of $w$ in $T$ is 

\begin{equation}
  IV_T(w) = f_w* | S_T(w) - S_{T'}(w) | 
\end{equation}

where $f_w$ is the frequency of appearance of $w$ in $T$. This value captures, in some way, whether a word
has a distribution in text that is not exactly uniform; an order of the word in the text that brings some sense to it. 
Ordering the words by their information value results in a list of the text's most representative words. In the context of our research, we call the first $n$ words of a text -with $n$ fixed-, ordered by it's information value, the \textit{top words} of the document.

\begin{equation}
  top\_words_{n}(T) = \{w | max(IV_T(w))\}
\end{equation}


\subsubsection{Wordnet synsets \& similarities}

Having the \textit{top-words} representation of a document, we were interested in measuring the distance or similarity of that text to different words. In order to address this problem we used different similarity functions defined over wordnet in the python's library nltk\footnote{link a nltk}. Wordnet is a lexical resource known as thesaurus, i. e, a data base of terms grouped as sets of synonyms (synsets), each of which have a definition and different semantic relations with others synsets (for example: hypernymy, hyponymy, meronymy, etc). The similarity functions are defined over this network of semantic relations.

This functions aren' t defined over words but over synsets. A synset, briefly, is an abstract notion of sense, meaning or concept that is independent from it's linguistic formulation or spelling. Using synsets instead of words, the wordnet thesaurus solves two problems of ambiguity limited to the linguistic domain: polisemy and synonymy. Polisemy occurs when a unique term may refer to more than one concept (for example, bank may refer to a financial institution or to a the land formation. In the other hand, synonymy occurs when different terms refer to the same concept (for example \sq{car} and \sq{automobile} refers, both, to the same concept).

The similarities defined in the nltk-api are the following:

\begin{itemize}
  \item Path Similarity: based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy.
  \item Leacock-Chodorow Similarity: based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. The relationship is given as -log(p/2d) where p is the shortest path length and d the taxonomy depth.
  \item Wu-Palmer Similarity: based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node). 
  \item Resnik Similarity: based on the Information Content (IC) of the Least Common Subsumer. Note that for any similarity measure that uses information content, the result is dependent on the corpus used to generate the information content and the specifics of how the information content was created.
  \item Jiang-Conrath Similarity Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer and that of the two input Synsets. The relationship is given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)).
  \item Lin Similarity: Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer and that of the two input Synsets. The relationship is given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2)).
\end{itemize}

\subsubsection{Word sense disambiguation}
\input{wsd}
\subsubsection{Top-sense definition}

In a first approach, we pretend to measure the distance between a document (represented by it's \textit{top words}) and different words. To address this problem using the wordnet distances between synsets described before, we modeled the notion of \sq{word} as the sum of all its synsets. The results were pretty demotivating, so we refocused our energy in order to define our problem in the space of word-senses (and no more in the space of words). 

Using the WSD algorithm described in the previous section, we defined the notion of \textit{top senses} of a document as the word-senses resulting from the disambiguation of the \textit{top words} of the documents obtained with Zannete's information value approach.

Given a document $T$ and an arbitrary integer $n$, the \textit{top senses} are defined as follow:
\begin{equation}
  top\_senses_{n}(T) = \{s | w \in top\_words_{n}(T) \wedge  s = wsd_T(w)\}
\end{equation}

Based on this definition, we evaluate the resultant \textit{top senses} of some texts and then check the results of the different similarities with well known concepts in the next sections.


The selected concepts are shown in the table below. We can distinguish three subsets in this selection: concepts related to practical situations like geopolitics and state management (like ...), concepts more related to the theoretical world (like...) and finally, concepts not related to Lenin works (like...). The motivation behind the selection of the two first groups is to check some easy hipotheses (such as "State and Revolution" talk a lot about "state" and "revolution", the motivation behind the third group is to chenk our methods for false positives, i.e., a high ranking in our metrics for a concept assumed not related to the works.

\input{handpicked_table}
\section{Results}

% \subsubsection{Top senses for selected works}

\input{results}

\subsubsection{Observations}

\section{Discussion}


\subsection{Top senses of a text}
The method used here, as explained before, was intended just to present the model of creating a list of top senses out of a list of top words. In spite of being quite elemental, the method worked fairly well with most texts, having some expected problems.

The main issues concern about Lesk Algorithm's flaws. One of these is the sensitivity of the context, for which the appearance or not of a single word in the gloss/context can radically change the selected sense for the word. Moreover, we strongly depend on glosses defined by Wordnet, which tend to be short.

Another issue is Wordnet itself. While it has overwhelming information for nouns and their relationships, the resources for verbs and adjectives are not as extensive. This fact was a problem not only for the construction of the glosses for Lesk, but also for measuring distances between synsets and works.

\subsection{Wordnet's synset distances}

As in the WSD problem, we also here just present a simple model to the problem of measuring a distance between a document and a -synset. The path similarity is a very simple similarity measure, which is based only in the distance between two synsets. Other 

For ``State and Revolution'', the distance worked as expected, matching with a high similarity those concepts highly related to the topics of the text, and putting down those unrelated. However, we must note that many synsets were extremely close of ``law.n.04'', which is a problem carried by a wrong disambiguation (the definition for it has to do with ``natural laws'', nothing to do with the text).

In ``Imperialism ... '', we suffered the appearance of a very general sense (universe.n.01) which made a nonrelated sense as hair to appear second in the ranking. Also, a very characteristic concept of the  as \textbf{capitalism} appears unrelated, because it is not very close (in the hypernyms/hyponyms taxonomy) to the top senses of this work. ``Materialism ... '' had the same problem but the other way around: \textbf{revolution} was just very close to \textbf{experience}, but had nothing to do with the text in general.

Many of these problems come from the fact that path similarity is not a good measure of semantic similarity. Of course, two senses which have a high path similarity are similar (they are very close in terms of hypernyms/hyponyms), but the other way round is not valid. For instance, meronimy is not taken into account in this metric.
 
Notice that, in order to use the path similarity, we had to select only noun senses for the top words. This is because verbs, adjectives and nouns are in different taxonomies in Wordnet, and hence are not comparable with this similarity. 





\section{Conclussion \& Future work}
The original scope of this investigation was the definition of a measure of distance between the works of a whole year and a concept; so we could measure whether the literary production of Lenin in some year was close to ``revolution'', ``theory'', etc.  With this purpose, we defined the \textbf{top senses} of a text, and measured the distance between them and other senses in a topological way.


\appendix[All tables]
% \input{top_senses_the_state_and_revolution}
% \input{top_senses_lenin_imperialism_the_highest_stage_of_capitalism}
% \input{top_senses_lenin_materialism_and_empirio-criticism}
% \input{top_senses_lenin_the_agrarian_programme_of_social-democracy_in_the_first_russian_revolution_1905-1907}
% \input{top_senses_lenin_what_is_to_be_done?}

\appendix[Entropy of a word in a text]
\label{appendix1}
Let's recall from \cite{DARWIN} and \cite{ENTROPIC} the concept of information value of a word.
Let $T$ be a text, which has been split into $P$ pieces $T_1, T_2, \dots, T_P$, $w$ be a word in $T$,
and $f_i$ the frequency of appearance of $w$ in $T_i$.

For each part $1 \leq i \leq P$, we could define:

\begin{equation}
  p_i = \frac{f_i}{\Sigma_{j=1}^{P}f_i}
\end{equation}

This quantity stands for probability of finding $w$ in $T_i$, given that it is present in the text. So, 
we define the Shannon information entropy of $w$ in $T$ as

\begin{equation}
  S = -\frac{1}{ln P}\sum_{i=1}^{P}p_i ln p_i
\end{equation}



\appendix[Optimal window size selection]

As \cite{DARWIN} mentions, we should consider the window size which maximizes the information value per word, for some sense of it.

Let's suppose we have a document, and a window size $W_S$ for which the information value algorithm returns a list $\{(w_1, v_1), (w_2, v_2), \dots ,(w_n, v_n) \}$ of word-information value pairs, in decreasing order of information value so that $v_1 \geq v_2 \geq \dots \geq v_n$. How would we assign some value to this pair of document-window size so we can choose the optimal window size?

One possibility would be to take the greater information value, in this case $v_1$. This approach, however, gives all the value just to a single word. Another chance would be to take the average of all the information values, but this leads to a lot of noise added because of words without any value.


The chosen approach is a tradeoff between both. We take the average not of all the words, but of a percentage of the most significative words. We call this percentage the \emph{sum threshold}. If $n$ is the number of words in the document, and $0 \leq ST \leq 1$ is the sum threshold, we calculate

\begin{equation}
  \frac{\sum_{i=1}^{[ST n]}v_i}{n} 
\end{equation}


The threshold value we used is $ST=0.001$, so it would consider only the $\frac{n}{1000}$ most significant words. The reason behind this was that it fitted the results from \cite{DARWIN} for the texts The origin of the species, Moby Dick, and  The Analysis of the mind.


\appendix[Old stuff of wordnet in spanish]
Consideremos el siguiente problema:
\newline
¿Cómo calcular la similaridad o distancia de un documento a un cierto concepto? 
\newline
El enfoque que utilizamos en este trabajo es el de realizar una representación del documento como un conjunto de palabras, y en base a esto usar una noción topológica de similaridad/distancia. La proyección del documento al espacio de palabras la realizamos utilizando la t\'ecnica de Information Value reci\'en descripta.
En este espacio, utilizamos Wordnet para definir una noción de distancia entre el conjunto de palabras que representan al texto y diferentes conjuntos representando conceptos.
\newline
Para un documento dado, el módulo de information value genera una lista con lo $n$ términos más importantes y su iv. Estos valores se normalizan -dividiendo por la suma de los valores-, obteniendo así uno de los inputs del proceso siguiente. Recordemos, nuestro objetivo es asignar una distancia entre un documento y un concepto. Digamos, decidir cuán presente está o cuán importante es un concepto en un documento.
\newline
Wordnet es una base de datos de palabras, agrupadas en conjuntos de sinónimos (synsets), cada uno de los cuales posee una definición y diferentes relaciones semánticas con otros synsets (por ejemplo: hipernimia, hiponimia, meronimia, etc). Sobre estas relaciones semánticas es posible definir distintas métricas de distancia y similaridad entre synsets (Ver [1])
\newline
Utilizando las métricas disponibles en el módulo nltk de python para wordnet definimos la similaridad entre dos palabras como la máxima similaridad entre todos los synsets de ambas. (**NOTA Parametrizable: puede ser sólo el primer synset de ambos y puede no ser la máxima similaridad sino un promedio. También para algunas métricas quedan afuera los synsets que no son sustantivos). 
\begin{multline}
S_w(w_1, w_2) = max \{ S_s(s_1, s_2) \}: s_1 \in synsets(w_1),\\ s_2 \in synsets(w_2)
\end{multline}

Para path similarity vale que $0 \leq S_w \leq 1$. 

Sobre $S_w$ (de word similarity) definimos la similaridad de un documento a un término como:

\begin{equation}
S_{dw}(d, w) = sum \{ S_w(d_i, w) \times p(d_i)  \} 
\end{equation}

donde las tuplas $(d_i , p(d_i))$ representan las palabras más importantes del documento $d$, y sus respectivos pesos normalizados. Vale que $0 \leq p(d_i) \leq 1$ pues está normalizado.

Por ejemplo $S_{dw}$(“Estado y Revolución”, “guerra”) = 0.110. $S_{dw}$ es una noción de cercanía entre un documento y una palabra.

Finalmente, definimos la noción de similaridad entre un año y una palabra como el promedio de las similaridades ($S_{dw}$) 
de los distintos documentos producidos durante ese año con la palabra en cuestión:
\begin{equation}
  S_{yw}(y, w) = mean \{ S_dw(y_i, w) \} : y_i \in docs(y) 
\end{equation}

Los resultados que presentaremos a continuaci\'on est\'an apoyados en distintas configuraciones de estas tres m\'etricas ($S_w$, $S_dw$ y $S_{yw}$).

% *SynsetAnalyzer
%   *Synset
%   *path, lch, wup
% *judge_word
% *judge_doc
% *judge_year
% Dado un conjunto de palabras, definimos la distancia de una palabra a este conjunto como 
% 




\begin{thebibliography}{10}
\bibitem{LENIN}
Marxists Internet Archive, {\em www.marxists.org} %, No estaria pudiendo definir bien esta referencia
\bibitem{DARWIN}
M.A. Montemurro and D. H. Zanette, {\em The statistics of meaning: Darwin, Gibbon and Moby Dick}, Significance, Dec. 2009, 165-169.
\bibitem{ENTROPIC}
M.A. Montemurro and D. H. Zanette, {\em Entropic analysis of the role of words in literary texts}, Adv. Complex Systems 5, September 27th 2001
\bibitem{MAS-ZANETTE}
\bibitem{WORDNET}
George A. Miller, {\em WordNet: A Lexical Database for English }, COMMUNICATIONS OF THE ACM November 1995/Vol. 38, No. 11
\bibitem{LESK}
M. Lesk. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone. In Proceedings of SIGDOC ’86, 1986.
\bibitem{BUDANITSKY}
Evaluating WordNet-based Measures of Lexical Semantic Relatedness
http://aclweb.org/anthology//J/J06/J06-1003.pdf
\end{thebibliography}
\end{article}
\end{document}





%\section{Acknowledgments}

%\section{Literature cited}

% \section{Analysis of almost sharp fronts}
% We begin our analysis on almost sharp fronts for the
% quasi-geostrophic equation recalling the notion of weak solution.

% For these solutions we have the following

% \appendix[Parallel calculus of information value]

% \begin{table}[h]
% \caption{Repeat length of longer allele by age of onset class.
% This is what happens when the text continues.}
% \begin{tabular}{@{\vrule height 10.5pt depth4pt  width0pt}lrcccc}
% &\multicolumn5c{Repeat length}\\
% \noalign{\vskip-11pt}
% Age of onset,\\
% \cline{2-6}
% \vrule depth 6pt width 0pt years&\multicolumn1c{\it n}&Mean&SD&Range&Median\\
% \hline
% Juvenile, 2$-$20&40&60.15& 9.32&43$-$86&60\\
% Typical, 21$-$50&377&45.72&2.97&40$-$58&45\\
% Late, $>$50&26&41.85&1.56&40$-$45&42\tablenote{The no. of wells for all samples was 384. Genotypes were
% determined by mass spectrometric assay. The $m_t$ value indicates the
% average number of wells positive for the over represented allele.}
% \\
% \hline
% \end{tabular}
% \end{table}


% \begin{table*}[ht]
% \caption{Summary of the experimental results}
% \begin{tabular*}{\hsize}
% {@{\extracolsep{\fill}}rrrrrrrrrrrrr}
% \multicolumn{3}{l}{Parameters}&
% \multicolumn{5}{c}{Averaged Results}&
% \multicolumn{5}{c}{Comparisons}\cr
% \hline
% \multicolumn1c{$n$}&\multicolumn1c{$S^*_{MAX}$}&
% \multicolumn1c{$t_1$}&\multicolumn1c{\ $r_1$}&
% \multicolumn1c{\ $m_1$}&\multicolumn1c{$t_2$}&
% \multicolumn1c{$r_2$}&\multicolumn1c{$m_2$}
% &\multicolumn1c{$t_{lb}$}&\multicolumn1c{\ \ $t_1/t_2$}&
% $r_1/r_2$&$m_1/m_2$&
% $t_1/t_{lb}$\cr
% \hline
% 10\tablenote{Stanford Synchrotron Radiation Laboratory (Stanford University,
% Stanford, CA)}&1\quad &4&.0007&4&4&.0020&4&4&1.000&.333&1.000&1.000\cr
% 10\tablenote{$R_{\rm FREE}=R$ factor for the $\sim 5$\% of the randomly
% chosen unique ref\/lections not used in the ref\/inement.}&5\quad &50&.0008&8&50&.0020&12&49&.999&.417&.698&1.020\cr
% 100\tablenote{Calculated for all observed data}&20\quad &2840975&.0423&95&2871117&.1083&521&---&
% .990&.390&.182&---\ \ \cr
% \hline
% \end{tabular*}
% \end{table*}

% \appendix[Estimating the Spectral Norm of a Matrix]
% In this appendix we describe a method for the estimation of the spectral norm
% of matrix $A$. The method does not require access to the individual
% entries of $A$; it requires only applications of $A$ and $A$* to vectors.
% It is a version of the classical power method. Its probabilistic
% analysis summarized below was introduced fairly recently in refs. 13
% and 14. This appendix is included here for completeness.


% \appendix
% This is an example of an appendix without a title.

% \begin{acknowledgments}
% algo
% This work was partially supported by 
% Spanish Ministry of Science and Technology Grant BFM2002-02042 (to D.C. and
% J.L.R.) and by National Science Foundation Grand DMS-0245242 (to C.F.).
% \end{acknowledgments}
